{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_to_wordlist( review, remove_stopwords=False ):\n",
    "\treview_text = BeautifulSoup(review).get_text()\n",
    "\treview_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\twords = review_text.lower().split()\n",
    "\tif remove_stopwords:\n",
    "\t\tstops = set(stopwords.words(\"english\"))\n",
    "\t\twords = [w for w in words if not w in stops]\n",
    "\treturn(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "raw_sentences = [\"this is anirudh\",\"hello who is this\",\"anirudh is speacking\",\"this is anirudh\"]\n",
    "wordlist = []\n",
    "for each in raw_sentences:\n",
    "    wordlist+=document_to_wordlist(each)\n",
    "c = Counter(wordlist)\n",
    "wordcloud = []\n",
    "for each in c:\n",
    "    temp = {}\n",
    "    temp['text'] = each\n",
    "    temp['size'] = c[each]\n",
    "    wordcloud.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.3982426   0.50300644  1.        ]\n",
      " [ 0.3982426   1.          0.12835482  0.3982426 ]\n",
      " [ 0.50300644  0.12835482  1.          0.50300644]\n",
      " [ 1.          0.3982426   0.50300644  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'links': [{'source': 0, 'target': 1, 'value': 0.3982425979778621},\n",
       "  {'source': 0, 'target': 2, 'value': 0.50300644050587495},\n",
       "  {'source': 0, 'target': 3, 'value': 0.99999999999999989},\n",
       "  {'source': 1, 'target': 2, 'value': 0.12835481850733338},\n",
       "  {'source': 1, 'target': 3, 'value': 0.3982425979778621},\n",
       "  {'source': 2, 'target': 3, 'value': 0.50300644050587495}],\n",
       " 'nodes': [{'name': 'this is anirudh'},\n",
       "  {'name': 'hello who is this'},\n",
       "  {'name': 'anirudh is speacking'},\n",
       "  {'name': 'this is anirudh'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(raw_sentences)\n",
    "matrix = (tfidf * tfidf.T).A\n",
    "print matrix\n",
    "force = {}\n",
    "force[\"nodes\"] = []\n",
    "force[\"links\"] = [] \n",
    "for each in raw_sentences:\n",
    "    temp={}\n",
    "    temp[\"name\"] = each\n",
    "    force[\"nodes\"].append(temp)\n",
    "for ((i,_),(j,_)) in itertools.combinations(enumerate(raw_sentences), 2):\n",
    "    temp = {}\n",
    "    temp[\"source\"] = i\n",
    "    temp[\"target\"] = j\n",
    "    temp[\"value\"] = matrix[i][j]\n",
    "    force[\"links\"].append(temp)\n",
    "force\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def load_word2vec(dir):\n",
    "    load_word2vec\n",
    "    word2vec = {}\n",
    "    for path in os.listdir(dir):\n",
    "        iword2vec = {}\n",
    "        #load the word2vec features.\n",
    "        with open(os.path.join(dir,path), 'r') as fin:\n",
    "            if path == 'vectors0.txt':\n",
    "                next(fin) #skip information on first line\n",
    "            for line in fin:\n",
    "                items = line.replace('\\r','').replace('\\n','').split(' ')\n",
    "                if len(items) < 10: continue\n",
    "                word = items[0]\n",
    "                vect = np.array([float(i) for i in items[1:] if len(i) > 1])\n",
    "                iword2vec[word] = vect\n",
    "\n",
    "        word2vec.update(iword2vec)\n",
    "        \n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vector= load_word2vec(\"static\\\\vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_to_wordlist( review, remove_stopwords=False):\n",
    "\t'''\n",
    "\t\tTakes a string and converts it to wordlist(list)\n",
    "\t'''\n",
    "\treview_text = BeautifulSoup(review).get_text()\n",
    "\treview_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\twords = review_text.lower().split()\n",
    "\tif remove_stopwords:\n",
    "\t\tstops = set(stopwords.words(\"english\"))\n",
    "\t\twords = [w for w in words if not w in stops]\n",
    "\treturn(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000000000002, 0.83357583365549581], [0.83357583365549581, 1.0000000000000002]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "raw_sentences = ['Hello, This is anirudh. I am studying in MSIT.','hELLO, THIS IS SNEHA STUDYING msit.']\n",
    "matrix = []\n",
    "matrix = []\n",
    "for each in range(len(raw_sentences)):\n",
    "    li=[]\n",
    "    for each1 in range(len(raw_sentences)):\n",
    "        li.append(0)\n",
    "    matrix.append(li)\n",
    "for i in range(0,len(raw_sentences)):\n",
    "    for j in range(0,len(raw_sentences)):\n",
    "        sen1 = raw_sentences[i]\n",
    "        sen2 = raw_sentences[j]\n",
    "        sen1_words = document_to_wordlist(sen1)\n",
    "        sen2_words = document_to_wordlist(sen2)\n",
    "        sen1_vectors = []\n",
    "        for each in sen1_words:\n",
    "            if each in word_vector:\n",
    "                sen1_vectors.append(word_vector[each])\n",
    "        sen1_vector = np.array(sen1_vectors).sum(axis=0)\n",
    "        sen2_vectors = []\n",
    "        for each in sen2_words:\n",
    "            if each in word_vector:\n",
    "                sen2_vectors.append(word_vector[each])\n",
    "        sen2_vector = np.array(sen2_vectors).sum(axis=0)\n",
    "        \n",
    "        matrix[i][j] = cosine_similarity(sen1_vector, sen2_vector)[0][0]\n",
    "print matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"nodes\": [{\"name\": \"Hello, This is anirudh. I am studying in MSIT.\"}, {\"name\": \"hELLO, THIS IS SNEHA STUDYING msit.\"}], \"links\": [{\"source\": 0, \"target\": 1, \"value\": 0.83357583365549581}]}\n"
     ]
    }
   ],
   "source": [
    "force = {}\n",
    "import itertools\n",
    "import json\n",
    "from collections import Counter\n",
    "force[\"nodes\"] = []\n",
    "force[\"links\"] = [] \n",
    "for each in raw_sentences:\n",
    "    temp={}\n",
    "    temp[\"name\"] = each\n",
    "    force[\"nodes\"].append(temp)\n",
    "for ((i,_),(j,_)) in itertools.combinations(enumerate(raw_sentences), 2):\n",
    "    temp = {}\n",
    "    temp[\"source\"] = i\n",
    "    temp[\"target\"] = j\n",
    "    temp[\"value\"] = matrix[i][j]\n",
    "    force[\"links\"].append(temp)\n",
    "graph = json.dumps(force)\n",
    "wordlist = []\n",
    "for each in raw_sentences:\n",
    "    wordlist+=document_to_wordlist(each)\n",
    "c = Counter(wordlist)\n",
    "wordcloud = []\n",
    "for each in c:\n",
    "    temp = {}\n",
    "    temp[\"text\"] = each\n",
    "    temp[\"size\"] = c[each]*20\n",
    "    wordcloud.append(temp)\n",
    "wordcloud = json.dumps(wordcloud)\n",
    "print graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000000000002, 0.83357583365549581], [0.83357583365549581, 1.0000000000000002]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[u'hello', u'this', u'is', u'anirudh'], [u'i', u'am', u'studying', u'in', u'msit'], [u'hello', u'this', u'is', u'sneha'], [u'i', u'study', u'at', u'msit']]\n",
      "[[ 0.99215674  0.          0.66143787  0.        ]\n",
      " [ 0.          1.          0.          0.44721359]\n",
      " [ 0.66143787  0.          0.99215674  0.        ]\n",
      " [ 0.          0.29408583  0.          0.98639399]]\n"
     ]
    }
   ],
   "source": [
    "texts = [] \n",
    "from gensim import corpora, models, similarities\n",
    "raw_sentences = ['Hello, This is anirudh', 'I am studying in MSIT','hELLO, THIS IS SNEHA','i STUDY AT msit.']\n",
    "matrix = np.zeros(shape=(len(raw_sentences), len(raw_sentences)))\n",
    "print matrix\n",
    "for each in raw_sentences:\n",
    "    texts.append(document_to_wordlist(each))\n",
    "print texts\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "lsii = models.LsiModel(corpus)\n",
    "\n",
    "matrix = np.zeros(shape=(len(raw_sentences), len(raw_sentences)))\n",
    "\n",
    "for i in range(len(raw_sentences)):\n",
    "    vec = corpus[i]\n",
    "    doc = raw_sentences[i]\n",
    "\n",
    "    vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "    vec_lsi = lsii[vec_bow]  # convert the query to LSI space\n",
    "\n",
    "    index = similarities.MatrixSimilarity(lsii[corpus])\n",
    "    sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "    cosine = list(enumerate(sims))\n",
    "    for j in range(len(raw_sentences)):\n",
    "        matrix[i][j] = cosine[j][1]\n",
    "print matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
